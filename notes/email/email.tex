\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{amssymb, amsmath, amsthm, enumitem, physics}
\usepackage[margin=0.75in]{geometry} % set page margins automatically
\usepackage{tcolorbox, bm}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{pdfpages}
\usepackage{qtree}

\newtcolorbox{mybox}[3][]
{
  colframe = cyan,
  colback  = white,
  coltitle = #2!20!black,
  title    = {#3},
  #1,
}

\pagestyle{fancy}

\newcommand{\pr}{\mathbb{P}}
\newcommand{\suml}{\sum\limits}
\newcommand{\intl}{\int\limits}
\renewcommand{\d}{\text{d}}
\begin{document}

\lhead{}
\rhead{}


\paragraph*{Terminology: }
\begin{itemize}
  \item Scene: a specific time step $t$ and its corresponding state $s_t$ (within an episode)
  \item Episode: Full sequence of scenes
  \item Memory: a 4-tuple ($s_t, a_t, r_t, s_{t+1}$)
  \item Trajectory: synonymous with episode. I.e., a trajectory is a sequence of memories that reaches a terminal state or some $t_{max}$.  
\end{itemize}

\hrule

\paragraph{System overview:} 

Actor-Critic  

\begin{itemize}
  \item Critic $\mathcal{C}$ is learning a value $V_\theta (s)$ that critiques the actor. The critic $\mathcal{C}$ uses attention to train on trajectories from prior experiences. This would happen through self-supervised ``leave one out'' predictions in simialar fashion to the BERT / GPT-2 text generation tasks.  
  \item TD-error is computed from $V_\theta (s)$:
    \begin{gather*}
      E_{TD} = r_t + \gamma V_\theta(s_{t+1}) - V_\theta(s_t) \\
      R_t = \suml_{k=0}^\infty \gamma^k r_{t+k}
    \end{gather*}
  
  \item Actor $\mathcal{A}$ (an MLP or something) acts based on sampled actions from a policy. The state $s_t$ and weights $\bm{\theta}$ are taken in by the actor $\mathcal{A}$ to output a probability distribution of actions $\pi_\theta (s_t)$. Sample from that probability distribution to select an action $a_t \sim \pi_\theta(s_t)$. 
\end{itemize}



\begin{algorithm}[H]
\SetAlgoLined
\caption{Attention assisted actor critic}

\For{\normalfont number of training episodes}{
  \While{\normalfont\text{not in terminal state }$(t<T)$ }{
		In state $s_t$, select action $a_t$ from the policy $\pi$: $a_t \sim \pi_{\theta}(s_t)$ \\
    Perform action $a_t$ to retrieve $s_{t+1}, r_t$. \\
    Store the memory $(s_t, a_t, s_{t+1}, r_t)$ to the replay buffer. \\
    \eIf % If
      {\normalfont\text{terminal state}}%, then 
      {Compute discounted rewards vector $\bm{R}$ with discount factor $\gamma$:  
        $ \bm{R} = \suml_{t=0}^{T} 
        \gamma^t r_t \hat{e_t} $ \\  
      Save the trajectory sequence $\bm{\tau} = [M_0, \cdots, M_{T}]$ \\
      break}  
    % else 
      (){$s_t \leftarrow s_{t+1}$ \\
      $t \leftarrow t+1 $}
    }

	\textbf{Method 1 (Contrastive):} Train critic with contrastive learning in order to update the policy.  \\
  Use attention in masked training task similar to BERT (Zhu et al., 2020).   \\
  Train by minimizing loss $\mathcal{L} = \mathcal{L}_{\text{RL}} + \lambda \mathcal{L}_{\text{ct}}$, where: \\
  $\bm{K} = (k_1, \ldots, k_T)$: Set of keys encoded from non-makes set $S$. I.e.,
  $k_i = \text{Enc}_{\theta_k}(s_i),  \forall s_i\in S$. 
  
  \begin{align*}
    &\mathcal{L}_{\text{ct}} 
      = \suml_{i=1}^{T} -M_i 
        \log \frac{\exp(q_i\cdot k_i / \omega)}{\suml_{j=1}^T
        \exp(q_i\cdot k_j / \omega)} , 
        \quad \omega\text{ is a hyperparameter} \\
    &\mathcal{L}_{\text{RL}} =  \ldots
  \end{align*}

  \textbf{Method 2 (Value estimates):}
  Compute $\bm{V} = [V_\theta(s_0), \ldots, V_\theta(s_T)]$ with states from the trajectory $\tau$. 
}

The gradient-based updates can use any standard gradient-based learning rule.
\end{algorithm}




Non-synthetic environments to try out: Robotic arm manipulation, Open AI Gym. 
\begin{itemize}
  \item 
\end{itemize}


Deep Self-Supervised Bidirectional Episodic D Attention Actor-Critic 

Deep Episodic D$\ldots$ Attention Actor Critic Self-Supervised

$D E A D A_cSS$

Stochastic Grad student descent 

\paragraph{Attention:}

Self-supervised ``leave one out" training on trajectory inputs. This is similar to how BERT / GPT-2 do self-supervised training on sentences to build context for words. We would do this with trajectories to build context on memories, where the attention mechanism predicts 

- Hidden layer embeddings are able to learn context-dependent information.

Training the attention part: 

Input time ($t$): 0 1 2 3 \_ 5 6 7 8       

For predicting t=4, we could input $s_4$, $a_4$, and predicts $s_5$ 

What's missing at $t=4$: $s_4, a_4$

Predicts $mathcal{R}$  -> expected reward $\gamma r_{4-8}$


\paragraph*{RL agent (live)}:

Then, when we give at seq like below:
Input time($t$): 0 1 2 3 4 \_

Right, we're in $s_4$ and want to pick $a_4$ and it would be helpful to know $r_4$ and $s_5$. 

$Q_{att}(s_t, a)$: attention-value of state-action pair

$pi(s_t)$ = $max_a [Q(s_4, a) + Q_{att}(s_4, a)]$ 

----

Generate N trajectories, where N is a hyperparameter. 

Attention trains on those trajectories in self-supervised setting to predict the expected reward?


Query is a seq. Tryign to figure out which keys you're most similar to. I'll give you a query (seq of tokens) and you'll tell me which keys most similar. Based on the similar b/w q and k,  $\to$ matrix products ($W_q$ $W_k$) attention project q's onto k's $\to$ Similarity metric is learned. Simulateneously, importance vector is learned. 

\end{document}